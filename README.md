# ğŸ« Ticket Triage Agent

An AI-powered support ticket classification and routing system built with LangGraph, FastAPI, and React. The agent intelligently analyzes support tickets, searches a knowledge base for similar issues, and provides structured classification with recommended next actions.

## âœ¨ Features

- ğŸ¤– **AI-Powered Classification**: Uses OpenAI GPT models for intelligent ticket analysis
- ğŸ” **Knowledge Base Search**: Semantic search across known issues with similarity scoring
- â¸ï¸ **Human-in-the-Loop**: Interrupts workflow to ask for clarification when needed
- ğŸ“Š **Real-time Streaming**: Live streaming of the triage process
- ğŸ¨ **Modern UI**: Clean, responsive interface with two-column layout
- ğŸ”„ **Stateful Workflows**: Resume interrupted workflows with additional context

## ğŸ—ï¸ Architecture

### Agent Design

The system uses **LangGraph** to orchestrate a multi-step agentic workflow:

```
User Query â†’ Search KB â†’ Analyze â†’ [Interrupt?] â†’ Classify â†’ Result
                â†“                       â†“
         Similarity Search      Need More Info?
                â†“                       â†“
         Top 3 Matches            Ask User â†’ Resume
```

#### **How the LLM is Used:**

1. **Analysis Node**: 
   - LLM evaluates if the ticket has sufficient information
   - Generates specific questions for vague tickets
   - Decision: Proceed or interrupt for more details

2. **Classification Node**:
   - LLM analyzes ticket + KB results + additional context
   - Extracts structured fields (summary, category, severity, issue_type, next_action)
   - Uses tool calling to ensure structured output

#### **Knowledge Base Search:**

- **Vector Similarity**: Uses OpenAI embeddings for semantic search
- **Cosine Similarity**: Compares ticket with known issues
- **Top-K Retrieval**: Returns top 3 most similar issues
- **Threshold-based Decision**: Similarity > 0.5 = known_issue

#### **State Management:**

- **LangGraph Checkpointer**: Stores workflow state with thread_id
- **Memory Persistence**: Enables resume from interruption
- **State Updates**: Additional user details merged into existing state

### Classification Output

Each ticket receives:
- **Summary**: 1-2 line concise description
- **Category**: Billing, Login, Performance, Bug, Question/How-To
- **Severity**: Low, Medium, High, Critical
- **Issue Type**: known_issue or new_issue
- **Next Action**: Specific recommendation (escalate, attach KB article, ask for logs)

## ğŸ“‹ Prerequisites

- **Python**: 3.10 or higher
- **Node.js**: 18.x or higher
- **npm**: 9.x or higher
- **OpenAI API Key**: Required for LLM and embeddings

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/Rohitw3code/ticket-triage-agent.git
cd ticket-triage-agent
```

### 2. Backend Setup

#### Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### Install Dependencies

```bash
pip install -r requirements.txt
```

#### Configure Environment Variables

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
PORT=8000
MAX_DESCRIPTION_LENGTH=5000
KB_PATH=kb/knowledge_base.json
```

#### Start the Backend Server

```bash
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

### 3. Frontend Setup

#### Navigate to Frontend Directory

```bash
cd frontend
```

#### Install Dependencies

```bash
npm install
```

#### Start Development Server

```bash
npm run dev
```

The UI will be available at `http://localhost:5173`

## ğŸ”Œ API Usage

### Endpoint: `/triage/stream` (POST)

Start a new ticket triage workflow with streaming response.

**Request:**

```bash
curl -X POST http://localhost:8000/triage/stream \
  -H "Content-Type: application/json" \
  -d '{
    "description": "Getting error 500 when trying to checkout on mobile app"
  }'
```

**Response:** (NDJSON stream)

```json
{"type": "status", "message": "ğŸ¤– I'm working on it... Please wait a moment while I embed the knowledge base", "thread_id": "abc-123"}
{"type": "node_start", "node": "search_kb", "message": "Executing node: search_kb"}
{"type": "kb_search_complete", "data": "Found related known issues:\n- ID: ISSUE-101 | Checkout error 500 on mobile | Similarity: 0.89\n  Recommended action: Escalate to payments team; link incident INC-2023-09-10"}
{"type": "node_complete", "node": "search_kb"}
{"type": "node_start", "node": "analyze", "message": "Executing node: analyze"}
{"type": "node_complete", "node": "analyze"}
{"type": "node_start", "node": "classify", "message": "Executing node: classify"}
{"type": "classification_complete", "data": {"summary": "User experiencing 500 error on mobile checkout, matches known issue ISSUE-101", "category": "Bug", "severity": "High", "issue_type": "known_issue", "next_action": "Escalate to payments team per ISSUE-101"}}
{"type": "node_complete", "node": "classify"}
{"type": "status", "message": "Triage complete"}
```

### Endpoint: `/triage/resume` (POST)

Resume an interrupted workflow with additional details.

**Request:**

```bash
curl -X POST http://localhost:8000/triage/resume \
  -H "Content-Type: application/json" \
  -d '{
    "thread_id": "abc-123",
    "additional_details": "The error happens specifically on iOS Safari, started yesterday after the app update"
  }'
```

### Endpoint: `/health` (GET)

Health check endpoint.

```bash
curl http://localhost:8000/health
```

## ğŸ§ª Testing

Run the test suite:

```bash
pytest tests/
```

Run with coverage:

```bash
pytest tests/ --cov=agent --cov=app
```

## ğŸ“‚ Project Structure

```
ticket-triage-agent/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ graph.py           # LangGraph workflow definition
â”‚   â”œâ”€â”€ orchestrator.py    # Agent orchestration & streaming
â”‚   â”œâ”€â”€ tools.py           # LangChain tools (search_kb, classify)
â”‚   â”œâ”€â”€ models.py          # Pydantic models
â”‚   â””â”€â”€ prompts.py         # Prompt templates
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py            # FastAPI application
â”‚   â””â”€â”€ config.py          # Configuration management
â”œâ”€â”€ kb/
â”‚   â”œâ”€â”€ knowledge_base.json # Known issues database (15 entries)
â”‚   â””â”€â”€ search.py          # Vector search implementation
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx        # Main React component
â”‚   â”‚   â”œâ”€â”€ App.css        # Styling
â”‚   â”‚   â””â”€â”€ main.tsx       # Entry point
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_triage.py     # Unit tests
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env                   # Environment variables (create this)
â””â”€â”€ README.md              # This file
```

## ğŸ¨ UI Features

- **Two-Column Layout**: Input on left, streaming results on right
- **Knowledge Base Display**: All 15 known issues visible with color-coded categories
- **Real-time Streaming**: See agent progress live
- **Interrupt Handling**: UI switches to response form when agent needs clarification
- **Mobile Responsive**: Stacks columns on smaller screens

## ğŸ­ Production Considerations

### Security

- **API Key Management**: 
  - Use environment variables for sensitive data
  - Consider AWS Secrets Manager or HashiCorp Vault in production
  - Rotate API keys regularly

- **Input Validation**:
  - Max description length enforced (5000 chars)
  - Sanitize user inputs to prevent injection attacks
  - Add rate limiting per user/IP

- **CORS Configuration**:
  - Current setup allows all origins (`allow_origins=["*"]`)
  - In production, restrict to specific domains
  - Example: `allow_origins=["https://yourdomain.com"]`

### Scalability

- **Async Processing**:
  - Already uses FastAPI async endpoints
  - Consider Celery for long-running tasks
  - Implement job queue for high traffic

- **Database**:
  - Current KB is in-memory JSON (15 entries)
  - For production, use vector database:
    - **Pinecone**: Managed vector search
    - **Weaviate**: Self-hosted option
    - **pgvector**: PostgreSQL extension
  - Add proper indexing for fast lookups

- **Caching**:
  - Cache KB embeddings (currently recomputed on every search)
  - Use Redis for session state instead of in-memory
  - Cache frequent query patterns

- **Load Balancing**:
  - Deploy multiple backend instances
  - Use NGINX or AWS ALB for load distribution
  - Implement health checks for auto-scaling

### Monitoring & Observability

- **Logging**:
  - Structured logging with correlation IDs
  - Log aggregation (ELK Stack, CloudWatch)
  - Track: latency, error rates, token usage

- **Metrics**:
  - Monitor API response times
  - Track OpenAI API costs per request
  - Alert on high error rates or latency

- **Tracing**:
  - LangSmith for LLM call tracing
  - OpenTelemetry for distributed tracing
  - Track full workflow execution paths

### Cost Optimization

- **LLM Costs**:
  - Current model: `gpt-4o-mini` (cost-effective)
  - Consider caching common queries
  - Implement response streaming to reduce user perception of latency
  - Monitor token usage per request

- **Embeddings**:
  - Cache KB embeddings on startup
  - Use smaller embedding models if accuracy permits
  - Batch embedding requests

### Error Handling

- **Graceful Degradation**:
  - Fallback to rule-based classification if LLM fails
  - Retry logic with exponential backoff for API calls
  - Circuit breaker pattern for external services

- **User Experience**:
  - Clear error messages for users
  - Automatic retry on transient failures
  - Fallback UI states

### Data Privacy

- **PII Handling**:
  - Identify and mask sensitive data in tickets
  - GDPR compliance for EU users
  - Data retention policies

- **Audit Trail**:
  - Log all triage decisions
  - Track who accessed what data
  - Enable compliance reporting

### Deployment

- **Containerization**:
  ```dockerfile
  # Example Dockerfile
  FROM python:3.10-slim
  WORKDIR /app
  COPY requirements.txt .
  RUN pip install --no-cache-dir -r requirements.txt
  COPY . .
  CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
  ```

- **CI/CD**:
  - Automated testing on every commit
  - Staging environment for validation
  - Blue-green deployment for zero downtime

- **Infrastructure**:
  - Container orchestration (Kubernetes, ECS)
  - Auto-scaling based on load
  - Multi-region deployment for HA

## ğŸ”§ Trade-offs & Time Constraints

### What Was Implemented

âœ… **Core Functionality**:
- LangGraph-based agentic workflow
- Real-time streaming with interrupts
- Human-in-the-loop for clarification
- Knowledge base search with embeddings
- Structured classification output
- Modern React UI with two-column layout

âœ… **Developer Experience**:
- Clear code structure and separation of concerns
- Type hints and Pydantic models
- Hot reload for development
- Comprehensive error handling

### What Could Be Improved (Given More Time)

âŒ **Performance**:
- KB embeddings are computed on every search (should cache on startup)
- No connection pooling for OpenAI API
- No request batching

âŒ **Persistence**:
- Thread state is in-memory (lost on restart)
- Should use Redis or database for production
- No conversation history storage

âŒ **Testing**:
- Limited unit test coverage
- No integration tests
- No E2E tests for UI

âŒ **Features**:
- No user authentication
- No ticket history or dashboard
- No analytics or reporting
- No webhook notifications for completion

âŒ **Knowledge Base**:
- Static JSON file with 15 entries
- No admin UI to add/edit issues
- No versioning or rollback
- Manual similarity threshold (0.5)

âŒ **UI/UX**:
- No loading states for individual components
- No retry mechanism in UI
- No export/share functionality
- Limited error state handling

### Design Decisions

1. **LangGraph over LangChain**: 
   - Enables stateful, interruptible workflows
   - Better for human-in-the-loop scenarios
   - More complex but more powerful

2. **Streaming over Batch**:
   - Better UX with real-time feedback
   - Users see progress immediately
   - More complex error handling

3. **In-Memory KB**:
   - Faster for small datasets (15 entries)
   - No database setup required
   - Not scalable beyond demo

4. **React over Server-Side Rendering**:
   - Better for interactive streaming UI
   - Client-side state management
   - Easier to develop and debug

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- Built with [LangGraph](https://github.com/langchain-ai/langgraph)
- Powered by [OpenAI](https://openai.com)
- UI framework: [React](https://react.dev) + [Vite](https://vitejs.dev)
- Backend: [FastAPI](https://fastapi.tiangolo.com)

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Built with â¤ï¸ by Rohit**
